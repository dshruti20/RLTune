name, utilization.gpu [%], utilization.memory [%]
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Using CUDA

Device: cuda

Running on a single GPU, no parallelism.
name, utilization.gpu [%], utilization.memory [%]
Tesla P100-SXM2-16GB, 100 %, 47 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Train Epoch: 1 [0/60000 (0%)]	loss=7.2227
Train Epoch: 1 [640/60000 (1%)]	loss=3.2480
Train Epoch: 1 [1280/60000 (2%)]	loss=2.7672
Train Epoch: 1 [1920/60000 (3%)]	loss=2.7469
Train Epoch: 1 [2560/60000 (4%)]	loss=3.3305
Train Epoch: 1 [3200/60000 (5%)]	loss=3.5256
Train Epoch: 1 [3840/60000 (6%)]	loss=2.6810
Train Epoch: 1 [4480/60000 (7%)]	loss=3.4182
Train Epoch: 1 [5120/60000 (9%)]	loss=2.4548
Train Epoch: 1 [5760/60000 (10%)]	loss=2.0602
Train Epoch: 1 [6400/60000 (11%)]	loss=2.0136
Train Epoch: 1 [7040/60000 (12%)]	loss=2.5970
Train Epoch: 1 [7680/60000 (13%)]	loss=1.5779
Train Epoch: 1 [8320/60000 (14%)]	loss=1.6110
Train Epoch: 1 [8960/60000 (15%)]	loss=1.6362
Train Epoch: 1 [9600/60000 (16%)]	loss=1.5624
Train Epoch: 1 [10240/60000 (17%)]	loss=1.2452
Train Epoch: 1 [10880/60000 (18%)]	loss=1.0118
Train Epoch: 1 [11520/60000 (19%)]	loss=1.2348
Train Epoch: 1 [12160/60000 (20%)]	loss=0.6877
Train Epoch: 1 [12800/60000 (21%)]	loss=0.8087
Train Epoch: 1 [13440/60000 (22%)]	loss=0.5135
Train Epoch: 1 [14080/60000 (23%)]	loss=0.3290
Train Epoch: 1 [14720/60000 (25%)]	loss=0.4578
Train Epoch: 1 [15360/60000 (26%)]	loss=0.4317
Train Epoch: 1 [16000/60000 (27%)]	loss=0.6095
Train Epoch: 1 [16640/60000 (28%)]	loss=0.3454
Train Epoch: 1 [17280/60000 (29%)]	loss=0.3571
Train Epoch: 1 [17920/60000 (30%)]	loss=0.4792
Train Epoch: 1 [18560/60000 (31%)]	loss=0.2818
Train Epoch: 1 [19200/60000 (32%)]	loss=0.1819
Train Epoch: 1 [19840/60000 (33%)]	loss=0.2431
Train Epoch: 1 [20480/60000 (34%)]	loss=0.2366
Train Epoch: 1 [21120/60000 (35%)]	loss=0.3707
Train Epoch: 1 [21760/60000 (36%)]	loss=0.2014
Train Epoch: 1 [22400/60000 (37%)]	loss=0.4224
Train Epoch: 1 [23040/60000 (38%)]	loss=0.1840
Train Epoch: 1 [23680/60000 (39%)]	loss=0.2316
Train Epoch: 1 [24320/60000 (41%)]	loss=0.0826
Train Epoch: 1 [24960/60000 (42%)]	loss=0.1935
Train Epoch: 1 [25600/60000 (43%)]	loss=0.1952
Train Epoch: 1 [26240/60000 (44%)]	loss=0.1441
Train Epoch: 1 [26880/60000 (45%)]	loss=0.1272
Train Epoch: 1 [27520/60000 (46%)]	loss=0.0638
Train Epoch: 1 [28160/60000 (47%)]	loss=0.1768
Train Epoch: 1 [28800/60000 (48%)]	loss=0.2553
Train Epoch: 1 [29440/60000 (49%)]	loss=0.0686
Train Epoch: 1 [30080/60000 (50%)]	loss=0.2892
Train Epoch: 1 [30720/60000 (51%)]	loss=0.1430
Train Epoch: 1 [31360/60000 (52%)]	loss=0.1293
Train Epoch: 1 [32000/60000 (53%)]	loss=0.1348
Train Epoch: 1 [32640/60000 (54%)]	loss=0.0877
Train Epoch: 1 [33280/60000 (55%)]	loss=0.0552
Train Epoch: 1 [33920/60000 (57%)]	loss=0.0291
Train Epoch: 1 [34560/60000 (58%)]	loss=0.2670
Train Epoch: 1 [35200/60000 (59%)]	loss=0.1197
Train Epoch: 1 [35840/60000 (60%)]	loss=0.0898
Train Epoch: 1 [36480/60000 (61%)]	loss=0.0847
Train Epoch: 1 [37120/60000 (62%)]	loss=0.0660
Train Epoch: 1 [37760/60000 (63%)]	loss=0.0946
Train Epoch: 1 [38400/60000 (64%)]	loss=0.1177
Train Epoch: 1 [39040/60000 (65%)]	loss=0.0512
Train Epoch: 1 [39680/60000 (66%)]	loss=0.1721
Train Epoch: 1 [40320/60000 (67%)]	loss=0.0351
Train Epoch: 1 [40960/60000 (68%)]	loss=0.1010
Train Epoch: 1 [41600/60000 (69%)]	loss=0.0568
Train Epoch: 1 [42240/60000 (70%)]	loss=0.1095
Train Epoch: 1 [42880/60000 (71%)]	loss=0.1401
Train Epoch: 1 [43520/60000 (72%)]	loss=0.1327
Train Epoch: 1 [44160/60000 (74%)]	loss=0.0511
Train Epoch: 1 [44800/60000 (75%)]	loss=0.1966
Train Epoch: 1 [45440/60000 (76%)]	loss=0.1414
Train Epoch: 1 [46080/60000 (77%)]	loss=0.1930
Train Epoch: 1 [46720/60000 (78%)]	loss=0.2318
Train Epoch: 1 [47360/60000 (79%)]	loss=0.0427
Train Epoch: 1 [48000/60000 (80%)]	loss=0.0379
Train Epoch: 1 [48640/60000 (81%)]	loss=0.1119
Train Epoch: 1 [49280/60000 (82%)]	loss=0.1035
Train Epoch: 1 [49920/60000 (83%)]	loss=0.1401
Train Epoch: 1 [50560/60000 (84%)]	loss=0.0205
Train Epoch: 1 [51200/60000 (85%)]	loss=0.0799
Train Epoch: 1 [51840/60000 (86%)]	loss=0.1010
Train Epoch: 1 [52480/60000 (87%)]	loss=0.1657
Train Epoch: 1 [53120/60000 (88%)]	loss=0.0196
Train Epoch: 1 [53760/60000 (90%)]	loss=0.0382
Train Epoch: 1 [54400/60000 (91%)]	loss=0.0245
Train Epoch: 1 [55040/60000 (92%)]	loss=0.0224
Train Epoch: 1 [55680/60000 (93%)]	loss=0.2370
Train Epoch: 1 [56320/60000 (94%)]	loss=0.1573
Train Epoch: 1 [56960/60000 (95%)]	loss=0.0489
Train Epoch: 1 [57600/60000 (96%)]	loss=0.1128
Train Epoch: 1 [58240/60000 (97%)]	loss=0.0703
Train Epoch: 1 [58880/60000 (98%)]	loss=0.0319
Train Epoch: 1 [59520/60000 (99%)]	loss=0.0803
 * Acc@1 97.550 Acc@5 99.960
name, utilization.gpu [%], utilization.memory [%]
Tesla P100-SXM2-16GB, 100 %, 42 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Tesla P100-SXM2-16GB, 0 %, 0 %
Train Epoch: 2 [0/60000 (0%)]	loss=0.0309
Train Epoch: 2 [640/60000 (1%)]	loss=0.0375
Train Epoch: 2 [1280/60000 (2%)]	loss=0.0443
Train Epoch: 2 [1920/60000 (3%)]	loss=0.1284
Train Epoch: 2 [2560/60000 (4%)]	loss=0.1353
Train Epoch: 2 [3200/60000 (5%)]	loss=0.1145
Train Epoch: 2 [3840/60000 (6%)]	loss=0.0928
Train Epoch: 2 [4480/60000 (7%)]	loss=0.0292
Train Epoch: 2 [5120/60000 (9%)]	loss=0.0953
Train Epoch: 2 [5760/60000 (10%)]	loss=0.0988
Train Epoch: 2 [6400/60000 (11%)]	loss=0.0290
Train Epoch: 2 [7040/60000 (12%)]	loss=0.1165
Train Epoch: 2 [7680/60000 (13%)]	loss=0.0338
Train Epoch: 2 [8320/60000 (14%)]	loss=0.0443
Train Epoch: 2 [8960/60000 (15%)]	loss=0.1700
Train Epoch: 2 [9600/60000 (16%)]	loss=0.0078
Train Epoch: 2 [10240/60000 (17%)]	loss=0.1172
Train Epoch: 2 [10880/60000 (18%)]	loss=0.2508
Train Epoch: 2 [11520/60000 (19%)]	loss=0.0128
Train Epoch: 2 [12160/60000 (20%)]	loss=0.0814
Train Epoch: 2 [12800/60000 (21%)]	loss=0.1009
Train Epoch: 2 [13440/60000 (22%)]	loss=0.0182
Train Epoch: 2 [14080/60000 (23%)]	loss=0.0203
Train Epoch: 2 [14720/60000 (25%)]	loss=0.0957
Train Epoch: 2 [15360/60000 (26%)]	loss=0.0770
Train Epoch: 2 [16000/60000 (27%)]	loss=0.0350
Train Epoch: 2 [16640/60000 (28%)]	loss=0.0139
Train Epoch: 2 [17280/60000 (29%)]	loss=0.0377
Train Epoch: 2 [17920/60000 (30%)]	loss=0.0982
Train Epoch: 2 [18560/60000 (31%)]	loss=0.0337
Train Epoch: 2 [19200/60000 (32%)]	loss=0.0765
Train Epoch: 2 [19840/60000 (33%)]	loss=0.0586
Train Epoch: 2 [20480/60000 (34%)]	loss=0.0218
Train Epoch: 2 [21120/60000 (35%)]	loss=0.0373
Train Epoch: 2 [21760/60000 (36%)]	loss=0.0139
Train Epoch: 2 [22400/60000 (37%)]	loss=0.0299
Train Epoch: 2 [23040/60000 (38%)]	loss=0.0323
Train Epoch: 2 [23680/60000 (39%)]	loss=0.0147
Train Epoch: 2 [24320/60000 (41%)]	loss=0.1056
Train Epoch: 2 [24960/60000 (42%)]	loss=0.0095
Train Epoch: 2 [25600/60000 (43%)]	loss=0.0726
Train Epoch: 2 [26240/60000 (44%)]	loss=0.0422
Train Epoch: 2 [26880/60000 (45%)]	loss=0.0840
Train Epoch: 2 [27520/60000 (46%)]	loss=0.0814
Train Epoch: 2 [28160/60000 (47%)]	loss=0.1288
Train Epoch: 2 [28800/60000 (48%)]	loss=0.0886
Train Epoch: 2 [29440/60000 (49%)]	loss=0.1715
Train Epoch: 2 [30080/60000 (50%)]	loss=0.0987
Train Epoch: 2 [30720/60000 (51%)]	loss=0.0978
Train Epoch: 2 [31360/60000 (52%)]	loss=0.0972
Train Epoch: 2 [32000/60000 (53%)]	loss=0.0503
Train Epoch: 2 [32640/60000 (54%)]	loss=0.0137
Train Epoch: 2 [33280/60000 (55%)]	loss=0.0691
Train Epoch: 2 [33920/60000 (57%)]	loss=0.0509
Train Epoch: 2 [34560/60000 (58%)]	loss=0.1523
Train Epoch: 2 [35200/60000 (59%)]	loss=0.0518
Train Epoch: 2 [35840/60000 (60%)]	loss=0.0212
Train Epoch: 2 [36480/60000 (61%)]	loss=0.1052
Train Epoch: 2 [37120/60000 (62%)]	loss=0.0621
Train Epoch: 2 [37760/60000 (63%)]	loss=0.0554
Train Epoch: 2 [38400/60000 (64%)]	loss=0.0683
Train Epoch: 2 [39040/60000 (65%)]	loss=0.0381
Train Epoch: 2 [39680/60000 (66%)]	loss=0.0669
Train Epoch: 2 [40320/60000 (67%)]	loss=0.0205
Train Epoch: 2 [40960/60000 (68%)]	loss=0.0820
Train Epoch: 2 [41600/60000 (69%)]	loss=0.1605
Train Epoch: 2 [42240/60000 (70%)]	loss=0.2550
Train Epoch: 2 [42880/60000 (71%)]	loss=0.0292
Train Epoch: 2 [43520/60000 (72%)]	loss=0.0276
Train Epoch: 2 [44160/60000 (74%)]	loss=0.0558
Train Epoch: 2 [44800/60000 (75%)]	loss=0.0875
Train Epoch: 2 [45440/60000 (76%)]	loss=0.1671
Train Epoch: 2 [46080/60000 (77%)]	loss=0.0303
Train Epoch: 2 [46720/60000 (78%)]	loss=0.0305
Train Epoch: 2 [47360/60000 (79%)]	loss=0.0518
Train Epoch: 2 [48000/60000 (80%)]	loss=0.0362
Train Epoch: 2 [48640/60000 (81%)]	loss=0.0547
Train Epoch: 2 [49280/60000 (82%)]	loss=0.0599
Train Epoch: 2 [49920/60000 (83%)]	loss=0.0332
Train Epoch: 2 [50560/60000 (84%)]	loss=0.0321
Train Epoch: 2 [51200/60000 (85%)]	loss=0.1766
Train Epoch: 2 [51840/60000 (86%)]	loss=0.0683
Train Epoch: 2 [52480/60000 (87%)]	loss=0.0270
Train Epoch: 2 [53120/60000 (88%)]	loss=0.0459
Train Epoch: 2 [53760/60000 (90%)]	loss=0.0552
Train Epoch: 2 [54400/60000 (91%)]	loss=0.0099
Train Epoch: 2 [55040/60000 (92%)]	loss=0.0083
Train Epoch: 2 [55680/60000 (93%)]	loss=0.0142
Train Epoch: 2 [56320/60000 (94%)]	loss=0.0369
Train Epoch: 2 [56960/60000 (95%)]	loss=0.0488
Train Epoch: 2 [57600/60000 (96%)]	loss=0.0508
Train Epoch: 2 [58240/60000 (97%)]	loss=0.0335
Train Epoch: 2 [58880/60000 (98%)]	loss=0.0093
Train Epoch: 2 [59520/60000 (99%)]	loss=0.1705
Traceback (most recent call last):
  File "/home/cc/MultiSourceML_Project/Training_Inference_Run/Training_LogCollection.py", line 325, in <module>
    main()
  File "/home/cc/MultiSourceML_Project/Training_Inference_Run/Training_LogCollection.py", line 312, in main
    test(args, model, device, test_loader, criterion, writer, epoch)
  File "/home/cc/MultiSourceML_Project/Training_Inference_Run/Training_LogCollection.py", line 45, in test
    output = model(data)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torchvision/models/resnet.py", line 158, in forward
    identity = self.downsample(x)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 193, in forward
    return F.batch_norm(
  File "/home/cc/miniconda3/envs/env_slurm/lib/python3.10/site-packages/torch/nn/functional.py", line 2812, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.99 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 14.34 GiB memory in use. Of the allocated memory 7.99 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
